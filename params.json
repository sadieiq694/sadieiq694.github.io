{
  "name": "Technical Paper for MIT Beaver Works Summer Institute, Sadie Allen",
  "tagline": "sadieiq694.github.io",
  "body": "#**Project Overview**\r\n\r\nOver the past four weeks, forty-six students with a strong interest in STEM from all over the country have participated in the pilot MIT Beaver Works Mini Grand Prix Challenge. Teams learned the basics of the algorithms involved in autonomous driving and used them to program RACECARs (Rapid Autonomous Complex-Environment Competing Ackermann-steering Robots), culminating in a final competition to demonstrate everything that was learned held on August 5, 2016.\r\n\r\n2016 is the pilot year for this program. Dr. Bob Shin of Lincoln Laboratories saw a need for an engineering and robotics summer program, and came up with the idea for the MIT Beaver Works Summer Boot Camp. The program is based upon the MIT class 6.141 RSS (Robotic Science and Systems) and essentially compresses the material of that course into a month long period. The amount of information covered made for a rigorous pace and exciting program for all involved.\r\n\r\nMonday through Friday, program participants would work in the Daniel Guggenheim Aeronautical Laboratory (MIT Building 33) from 9:00am to 5:00pm. Most mornings started with a technical lecture to prepare students for the lab that immediately followed. Every day right before lunch there was also a guest seminar speaker, usually a STEM professional. On Tuesdays and Thursdays after lunch Ms. Jane Connor would give a lecture on communication and teamwork. The rest of the day would be devoted to more lab time during which students worked in groups of five to six writing and testing code for the RACECARs.\r\n\r\nDuring the first week, the focus was on learning the basics of ROS, Linux, and Python, and taking the first steps in applying these skills through basic movement in our RACECARS. Students also learned about control systems that could be used to make the cars accomplish specific goals. In the final challenge cars raced side by side following either the left or right wall as fast and efficiently as possible.\r\n\r\nThe content during the second week primarily concerned with “blob detection,” or, recognizing different colored sheets of paper and making decisions based on the color, shape, or size of the blob. Groups utilized the RACECAR’s ZED camera to capture images and used open CV to manipulate them. The week finished with a difficult challenge in which cars had to detect a red or green blob, drive towards it until they were a certain distance away, then, based on the color of the blob, turn left or right and wall follow.\r\n\r\nThe third week was originally supposed to cover localization and mapping, but due to problems with software plans changed slightly. The technical lectures still covered those topics, but the labs focused on more reactive navigation; we built upon what we learned in the previous two weeks and programmed our cars to drive around a pen and avoid obstacles without use of a map, while detecting different colored blobs. In the final challenge, the RACECAR’s had to drive around autonomously for two minutes and detect as many red, green, yellow, and blue blobs as possible while avoiding any collisions.\r\n\r\nFinally, in week four teams prepared for the exhibition day and grand prix race. Teams were to use code from every week to showcase what they learned. Race day consisted of two technical challenges; the first was a “move and explore challenge” similar to week three’s final task, and the second was “make the correct turn,” similar to week two’s. In addition to these challenges, the cars had to complete three time trials, one heat race against two other cars, and a final race with all nine cars on the grand prix race track.\r\n\r\nBy the end of the program, students had gained confidence in ROS, Python, and Linux. They learned about and implemented many algorithms that are used by companies like Tesla and Google to program real autonomous vehicles. In addition, participants gained valuable robotics experience and learned about how to be successful when working in groups. The seminars from experts in various STEM fields showed students the many directions one can go if he or she is interested in engineering and technology, and it was truly an amazing opportunity to get to hear all of them speak. The MIT Beaver Works Summer Institute was an incredible opportunity that greatly benefited all who had the privilege to attend.\r\n\r\n************************************************************************************************************************\r\n\r\n#**Week One: Technical Work**\r\n\r\n##**Goals**\r\n\r\n  The first week of the program served as a period to set the foundation of ROS, Linux, and Python\r\n  knowledge, as well as acquaint students with the RACECAR platform. Everyone started at a slightly\r\n  different level (some students had prior Python experience, everyone got through different amounts\r\n  of the pre-program coursework), and for the first few days of week one the instructors were\r\n  primarily concerned with getting all students on a level that would allow them to succeed in the\r\n  weeks to follow.\r\n\r\n  In week one groups also were given the task of programming their RACECARs to wall follow -- or,\r\n  drive forward and stay a specific distance away from a wall -- for ~30 feet as quickly as possible.\r\n  The cars had to be able to follow both the left and right walls and switch between the two on\r\n  command. Since all of the cars had the same speed cap (2 meters per second), the only way\r\n  for teams to differentiate themselves was by having a better algorithm (in terms of computation\r\n  time and oscillation levels).\r\n\r\n##**Approach**\r\n\r\n###The RACECAR\r\n\r\nIn order for groups to control their RACECARs, it was first crucial for them to understand them and all of their components.\r\n![RACECAR](https://cloud.githubusercontent.com/assets/18174572/17645837/ae61aabe-617e-11e6-96b2-f528a82376e1.png)\r\nFigure 1: Labeled illustration of the RACECAR used in this course (1); note that the cars used in the Beaver Works Summer Program did not have active stereo cameras, and the passive cameras were mounted above the LiDAR where the active camera appears in this image.\r\n\r\nThe robots used in this program were equipped with an advanced supercomputer and sensors that provided the data necessary for the algorithms to work. (1)\r\n  * **Chassis:** The basic frame of the vehicle is the Traxxas Rally 74076, a 1/10 scale RC car with four wheel drive and    Ackermann front wheel steering. It is capable of speeds up to 40 miles per hour, but this program's purposes this to 2 m/s (or 4.5 mph).\r\n  * **Processor:** The computer aboard the car is the Nvidia Jetson TX1 embedded systems module running Ubuntu for ARM. It is equipped with 256 CUDA cores that deliver over 1 TeraFLOPs of performance (2). The large number of GPU cores allows the TX1 to perform many parallel operations at the same time, making it extremely fast and efficient. The speed definitely comes in handy for the RACECAR's purposes; data could be processed and commands could be given in real time, which is very important for the reactive navigation the cars did for the vast majority of the time.\r\n  * **2D LiDAR:** Each car also had a 2D Hokuyo UST 10LX scanning laser rangefinder. The LiDAR has a range of .06m to ~10m and an accuracy of +/-40mm. It has a scan angle of 270 degrees and a speed of 40Hz. The laser scanner was one of the most frequently used scanners throughout the duration of the program. It scans 270 degrees and splits it into 1081 points, returning a list of the distances (in meters) from the car that each of the points are. Specifically, it does this by sending out beams of light (lasers) and measuring the time it takes for them to come back. The following formula:\r\n\r\n                                        D = ct/2                                                    (eq. 1)\r\n\r\nWhere c = speed of light and t = time, can then be used to determine the distance (D). This data was vital as most steering decisions the car made (all of them during week one) were based upon it.\r\n\r\n![Hokuyo](https://cloud.githubusercontent.com/assets/18174572/17646411/ba69fbc4-6195-11e6-8ebb-77e65e6b56e5.png)\r\n\r\nFigure 2: Hokuyo LiDAR and diagram depicting its range (1)\r\n  * **IMU:** A Sparkfun 9 Degrees of Freedom \"Razor\" Inertial Measurment Unit was also present on each vehicle. The IMU has three sensors: a MEMS (MicroElectroMechanical System) three-axis accelerometer that assesses translational acceleration, a MEMS three-axis gyroscope that measures Coriolis force, and a three-axis Anisotropic MagnetoResisteance (AMR) magnetometer. The IMU is used to compute the current acceleration of the car.\r\n  * **Passive Stereo Camera:** Unlike the diagram pictured above, the cars utilized during the Beaver Works program had only one camera: a Sensorlabs ZED Passive Stereo Camera with automatic depth perception from .7 to 20 meters and a field of view of 110 degrees. Although groups never utilized the depth perception functionality of the camera, it is made possible because the camera actually consisted of two separate cameras that would record the same image, use image processing software to identify matching points in the images, and then solve the translation between them to obtain depth data. The farther apart an object seems to the two cameras, the closer it is.\r\n\r\n![Car and Controller](https://cloud.githubusercontent.com/assets/18174572/17651127/cbe4150e-622d-11e6-96c7-6885b09627ee.png)\r\n\r\nFigure 3: RACECAR and remote controller (1)\r\n\r\nIn addition, cars came with a controller (see above) that could be used to command the car's movements. Before ever running a program, the controller would be connected to the car. The left trigger worked as a \"deadman switch\" which would immediately stop the car upon being pressed. This was a safety measure taken to protect the RACECARs. A human was always in control and could stop the car from colliding with an obstacle if a program did not run as planned.\r\n\r\n###Basics of ROS\r\n\r\nStudents also had to have a basic understanding of the Robot Operating System (ROS) in order to complete any of the assigned tasks.\r\n\r\nROS is an operating system that is used for robots. The overarching goal of the system is to support code reuse in robotics; before ROS, people had to start from scratch whenever they wanted to program a new robot, even though a lot of the code that they wrote had already written before (4). Because ROS is open source, meaning all source code is made available, ROS saves software developers a huge amount of time because they can reuse code written by others. This allows progress in robotics to be made much more quickly. ROS is a powerful, flexible (supports multiple languages and multi-machine systems) tool that is widely used in academia and industry and is supported by the Open Source Robotics Foundation (OSRF). (5)\r\n\r\nOne aspect of ROS that makes it so effective for robotics software development is how its design supports modularity. Most tasks that robots have to accomplish are complex and have many different components. Completing each component linearly is time consuming and does not take advantage of the Jetson TX1's many GPU cores. Luckily, ROS makes it very simple to employ a modular software design by allowing elaborate tasks to be broken into simpler pieces. Then, multiple parts of the problem can be solved at the same time, allowing programs to run much more quickly. (5)\r\n\r\nROS utilizes modularity by organizing programs into nodes. Nodes are processes performing specific computations in the ROS system. There can be as few as one and as many as one hundred in a single program. The nodes have to be able to communicate with each other, and they do that using messages and topics. Messages are strictly typed packets of data sent between ROS nodes. They are sent over topics: unidirectional communication links between ROS nodes. When a node wants to send data, it must **publish** a message to a specific topic, and the node that wants to receive that data must **subscribe** to that topic. Each topic is linked to a single message type. It is important to note that one or more nodes may publish messages to a topic and one or more nodes can subscribe to any topic. In addition, an individual node can publish or subscribe to as many topics as it needs to. The ROS Master process is responsible for linking nodes together. It coordinates the peer-to-peer communication links between nodes. Nodes that wish to publish advertise to the ROS Master on the desired topic. Nodes that wish to subscribe to that topic tell the ROS Master, and it establishes a connection between the two nodes. (5)\r\n\r\n\r\n###Control Systems\r\nTo help us with this first challenge, a robotics software engineer from NASA's Jet Propulsion Laboratory (JPL), Kyle Edelberg, gave technical lectures on control systems that we could use.\r\n\r\nA simple definition of a control system is something that takes a system (specifically the RACECAR for the program's purposes) from state A to state B, the desired state. The controller is implemented in the software as the program(s) that tell the system what to do, and the system is the hardware (the RACECAR for Beaver Works' purposes). (6)\r\n\r\nThere are two basic types of control that a system can use: open loop control and closed loop control. (6)\r\n\r\nOpen loop control is the more primitive of the two types of control system. The controller tells the system to do something and receives no feedback to let it know how close it is to completing the task. An example of open loop control is if a RACECAR had to move 10m and the top speed was 2m/s, so the program told the car to run at full speed forward for five seconds. The car would run for the five seconds and stop, but the controller would have no idea if it was successful. In addition, this method does not account for many errors such as the wheels sticking, slipping, or being misaligned, and if any of that happened, the program would not be able to adjust and achieve the desired state. Open loop control is not usually used, but sometimes when the reaction time for the controller to adjust its plan would be too slow anyway or during testing it is used. (6)\r\n\r\nClosed loop control is usually preferred over open loop. In this control method, the controller receives feedback from the system to help it achieve the desired value. Closed loop control is what groups used for their wall following algorithms. (6)\r\n\r\n![Open Loop Control (Wall Follow)](https://cloud.githubusercontent.com/assets/18174572/17651717/ff97b6e6-623a-11e6-8448-2cbcaff08204.png)\r\n\r\nFigure 3: Diagram depicting closed loop control for wall following (6)\r\n\r\nThe above diagram illustrates how closed loop control works for wall following. A desired distance (Ddes) is inputed into the controller as well as the current distance(D) from the wall, which the system's sensors send to the controller. The controller then subtracts the current distance from the desired distance to obtain the error, which the system wants to  be as close to zero as possible. Based on the error, the controller sends steering commands to the system, which sends more data back to the controller in a continuous loop. (6)\r\n\r\nMr. Edelburg introduced us to two types of closed loop control that could be used: bang bang control and PID control.\r\n\r\nBang bang control is the simplest form of closed loop control. It looks at the error value (Ddes - D). If it's positive, the car is too close to the wall, so the controller tells the car to steer as far as possible away from the wall (if following the right wall, the steer command would be -1, which is a full left). If the error value is negative, the car is too far away from the wall so the program tells the car to steer all the way towards the wall (again, in a right wall following scenario the command would be 1, full right). Bang bang control is effective, but causes significant oscillations in the car's forward motion. This means the car is traveling farther than if it were able to drive straight, which is not ideal when teams wanted their cars to move as quickly as possible. (6)\r\n\r\nPID control, or, Proportional Integral Derivative control, is another fairly simple control scheme that makes use of the following equation (6):\r\n\r\n                              u = kp*e + ki*∫e + kd*ẻ                                               (eq. 2)\r\n\r\nWhere u is the steering command (a number between -1 and 1, or, full left and full right), kp, ki, and kd are experimentally determined constants, e is the error, ∫e is the summation of the errors (integral with respect to time), and ẻ is the derivative of the errors. Each of the terms of this equation has a specific goal. The kp term initially brings the error value close to zero, the ki term drives it all the way down, and the kd value helps with stability, i.e., minimizes oscillations (6). Most groups opted to use PID control in week one's final challenge.\r\n\r\n![PID](https://camo.githubusercontent.com/bbede27c5fa69f4764cf2727cb42740aa7d46b5b/68747470733a2f2f75706c6f61642e77696b696d656469612e6f72672f77696b6970656469612f636f6d6d6f6e732f332f33332f5049445f436f6d70656e736174696f6e5f416e696d617465642e676966)\r\n\r\nFigure 4: Animation showing the effects of changing PID values (3)\r\n\r\n###Challenge Approach\r\n\r\nAfter gaining the necessary background through lectures and short labs, groups were ready to start working on the implementation of a wall following node. When deciding our approach, my team prioritized learning and obtaining experience with the robot above all else, which is reflected in all of the choices that were made.\r\n\r\nIn order to get more practice writing nodes, my group decided to first write one that used the bang bang control system and then write a PID node. Although this approach left less time to tweak PID values to perfection, my team decided it was worth it in the long run to have a better understanding of nodes and ROS in a general sense.\r\n\r\nAnother decision our group made going into week one's challenge was to not delegate tasks; for the most part we all worked on everything together. Again, working as a full group was not the most efficient way to go about the task, but it made it much easier to ensure that all group members understood what was going on throughout the whole process.\r\n\r\nThe final decision the group had to make before starting to code was how to interpret the laser data. This at first seems like a straightforward problem; find the point on the scan that is at a 90 degree angle with the front of the car and use that distance as th distance from the wall. However, with the car steering and very rarely driving exactly parallel to the wall, this would not always be the correct distance. The car would sometimes be much closer to the wall than that single point would appear. To combat this there were two options. The first was to use trigonometry by taking two points at a 30 degree angle and using law of sines and law of cosines to solve for the actually distance from the wall. The second was to take a large swathe of points and find the one with the smallest value. This would be the minimum distance between the car and the wall. Our group chose to go with the second approach because it was simpler to program and did not make use of computationally expensive trigonometric functions. In addition it was more reliable than relying on just two points; if the car reached a corner, the point farther out in front could get an extremely high value that would cause the calculated distance to be incorrect. By looking at the smallest value within a range, the chance of being thrown by massive outliers is virtually eliminated.\r\n\r\n##**Process**\r\n\r\nThe process of implementing a wall follow algorithm was cyclical. Our team would write code, test it to see what it would do, change the code, and retest. To get started we used node templates found on the ROS wiki. This gave us a good idea of the format we needed to use and then we just filled it in with our own code. There were a few bumps along the way as this was our first time writing a node but we worked methodically through everything and made sure to thoroughly comment the code to make it understandable.\r\n\r\nThe bang bang controller followed solely the left wall and had a scan range of 540 to 930. The node 'wall_bang' subscribed to the 'scan' topic to obtain the LiDAR data and published drive commands to the '/vesc/ackermann_cmd_mux/input/navigation' topic. Once the code was functional, it required very little testing because there were no values to tweak. The one thing we added on top of basic bang bang control was a threshold value; if the error was small enough (specifically, if the absolute value of the error was less than .03 meters), the node will publish a steering command of zero, which tells the car to go straight. The bang bang controller was functional but oscillated a lot.\r\n\r\nAfter successfully coding the bang bang controller, we moved on to PID. One thing we decided during the coding process was to turn PID control into PD control; the I term is just serves to bring the error value closer to zero, and in our case, that was not important. The car just had to follow a wall as quickly as possible; the exact distance it was from the wall, which is the only thing that implementing the I portion of PID control would change, did not matter. By scratching it we saved computational time and had more time to test PD values.\r\n\r\nWriting the PD node did not take that long because we just had to alter and add a few things in the bang bang algorithm.  We completed the process in steps. First, we just had a P value. Then, once we had tested that and made sure that part was functional, we added in the derivative element. Breaking the code down into testable parts makes the debugging process much more efficient because the sources of errors are easier to locate. Of course, once the D section was added in, the kp value had to be altered. Our group spent a lot of our lab time testing different kp and kd values to optimize the algorithm. On Kyle Edelberg's recommendation, we started with the kp value very low and gradually incremented it, then did the same with kd. The values we settled on were .7 for kp and .6 for kd.\r\n\r\nThis algorithm could follow either the left or the right wall depending on what button was pressed on the controller. The 'a' button made it follow the left wall and 'b' made it follow the right. If no buttons were pressed, it would default to following the right wall. For our scan ranges, we used 200 to 540 to for the right and 540 to 900 for the left.\r\n\r\nAnother feature we added to this node was a 'safety.' We took a small scan directly in front of the robot (from 525 to 555), and if the minimum distance to a point within that range was less than .5 meters, the speed of the robot would be set to -.1. We used -.1 instead of 0 to make the robot stop more quickly. Although the safety could have been implemented in its own node, it seemed simpler to us to just make it part of the wall following node to avoid dealing  with node hierarchy.\r\n\r\nFull code for both algorithms can be found in the scripts folder of this github page.\r\n\r\n##**Results**\r\n\r\n  The final challenge was set up as a \"drag race.\" In the first round, cars had to complete two time\r\n  trials: one following the left wall and one following the right. Each car's fastest time was used\r\n  to place them in the second round. The fastest car got a bye to the next round, and the rest of the\r\n  nine cars were paired off and raced head to head. The races were best two out of three and the\r\n  fastest of each of these pairings moved on to the final round, as well as the fastest losing car\r\n  from all the rounds. In the final round three pairs of cars again raced best two out of three, this\r\n  time vying for gold, silver, and bronze medals.\r\n\r\n  In the time trials, car 45, my group's car, came in fourth with a time of 8.645 seconds. In round\r\n  two, we were up against car 34, which did not complete any successful runs in the three attempts,\r\n  so we advanced to the final round. In the final round the start was slightly different from in the\r\n  time trials; in the time trials, the timer would start as soon as the car moved across the start\r\n  line, but in the finals it would start as soon as the start signal was given. The cars take time\r\n  to get started, so this change was reflected in the final results. Our best time was 9.58 seconds,\r\n  almost a full second slower than in the time trials. We placed sixth overall in the final round.\r\n\r\n************************************************************************************************************************\r\n\r\n#Week Two: Technical Work\r\n\r\n##Goals\r\n\r\nAfter week one, students felt more comfortable with the cars and basic motion commands. So, for week two, groups built upon these skills by combining them with basic computer vision.\r\n\r\nLike the previous week, there were two main goals during the week. The first was to  learn about image processing and become comfortable using OpenCV, an open source computer vision software library (7). The final challenge goal incorporated things learned during both weeks. The cars had to be able to detect a brightly colored piece of construction paper, or, 'blob', and drive toward it using visual servoing. This blob was either red or green, and once the robot got a certain distance away from it (approximately one meter), it had to make a decision based upon this color. If the blob was green, the car had to turn right and follow the left wall to a finish line. If it was red, the car had to turn left and follow the right wall to a different finish line. Again, teams were to complete these tasks as quickly as possible.\r\n\r\n##Approach\r\n\r\nAgain, some background knowledge is necessary before my group's approach can be described.\r\n\r\n###Computer Representation of Images\r\n\r\nA computer deals with color images as 2D pixel arrays where each pixel encodes a color (8). There are several ways in which computers can represent colors, but in the Beaver Works course students mostly worked with RGB and HSV formats. \r\n\r\nThe human eye perceives color using specialized cells called cones. There are three different types of cones in our eyes that all respond most strongly to different wavelengths of light; red, green, or blue (9). RGB representation is based off the human visual system. Each pixel in the 2D array that represents an image encodes a color with the triplet (R, G, B). Because color pixels are typically encoded in 24 bits, or, 8 bits per color, the R, G, and B components take values between 0 and 255 (8). (0, 0, 0) encodes black, (255, 255, 255) encodes white, and any triplet in which all three values are the same represents gray. (255, 0, 0) would be a 'perfect' red.\r\n\r\nAn alternative to RGB is the HSV color model. This system also uses triplets to represent colors, but the three values represent hue, saturation, and value. Hue is an angle out of 180 degrees. 0 degrees is red, 60 degrees is green, and 120 degrees is blue. Saturation is out of 255 and represents the intensity of the color. Value is also on a scale of 0 to 255 and is a measure of the brightness of a color (8).\r\n\r\n![HSV Chart] (https://cloud.githubusercontent.com/assets/18174572/17703787/f45c870e-63a0-11e6-86b2-d12b2cd06902.png)\r\n\r\nFigure 1: Diagram showing how colors change in accordance with HSV values (10)\r\n\r\nThe HSV model works better for blob detection because it is much more robust under illumination changes.\r\n\r\n###Intro To OpenCV Tools\r\n\r\nStudents used OpenCV to process the images received from the ZED camera. Specific tools in the library that were particularly useful for week two's challenge were:\r\n\r\n* **Creating A Mask**: Using cv2.inRange, one can select upper and lower bounds for color values and create a \"mask\" that highlights a specific color. This method can be used to initially identify blobs of specific colors. (7)\r\n\r\n![mask](https://cloud.githubusercontent.com/assets/18174572/17709879/deab533c-63b8-11e6-95a7-1b490beab197.png)\r\n\r\nFigure 2: Side by side of original image and binary mask for red range (11)\r\n\r\n* **Creating a Contour**: Contours are essentially curves that join all continuous points along a boundary having the same color. They work best on binary images, which is why a mask is used to isolate the specific colors. By using the function cv2.findContours, one can identify contours using the binary image. The contours can then be applied to the original image using cv2.drawContours (7).\r\n \r\n![contour](https://cloud.githubusercontent.com/assets/18174572/17710445/4aed708c-63bb-11e6-83fa-fa6570613979.png)\r\n\r\nFigure 3: Image with contours applied (11)\r\n\r\n* **Using Moments**: Moments (cv2.moments) can be used to find the center of contours, which is useful to know when visual servoing (7). One can compare the x coordinate of the center of a contour (that outlines the desired blob) to the x coordinate of the center of the camera's frame and make steering decisions based on the difference. \r\n\r\n![center](https://cloud.githubusercontent.com/assets/18174572/17711449/ac908e7e-63bf-11e6-8275-5bdbf1ecbd89.png)\r\n\r\nFigure 4: Image with center identified (11)\r\n\r\n* **Creating a Bounding Rectangle**: cv2.boundingrect, as the name suggests, draws a rectangle around a contour. It is also one of several functions that can be used to determine the rough area of a blob, as it computes both height and width (7). However, this approach to finding the area is only accurate when the blob is rectangular in shape.\r\n\r\n![bounding rectangle](https://cloud.githubusercontent.com/assets/18174572/17712492/28f3e264-63c4-11e6-9bed-37eb7119b0af.png)\r\n\r\nFigure 5: Side by side view of red contour and bounding rect based off of contour (12)\r\n\r\nThese tools were primarily what groups used to work with blobs for week two's challenge.\r\n\r\n###Custom Messages\r\n\r\nCustom messages were another important tool introduced to students. Sometimes it is necessary to communicate multiple pieces of information between nodes of varying different types (13). This is made possible by custom message types. An example is as follows:\r\n\r\n      Header header\r\n      std_msgs/String color\r\n      std_msgs/Float64 size\r\n      geometry_msgs/Point location\r\n\r\nThis is the message type my group used in the week two challenge to communicate information about the blob. The custom message allowed the color, size, and location of the blob to be sent all together instead of individually. \r\n\r\n###Challenge Approach\r\n\r\nWe decided to handle the challenge tasks with two nodes. One would handle vision and blob detection and one would handle motion commands. By writing fewer nodes, we hoped to save time that we could spend working with image processing, which proved to be difficult in the preliminary labs. \r\n\r\nThe blob detection node would determine the size, location, and color of the blob and pass it to the motion control node, which would use that information to make all driving decisions.\r\n\r\nIn terms of task delegation, our group split so some of us were working on blob detection and others worked on motion commands. Again, time played a big role in this decision. It would have been preferable for the entire group to work through everything together to ensure unanimous comprehension of the code, but because we went with a \"divide and conquer\" approach. \r\n  \r\n##Process\r\n\r\n###Blob Detection Node\r\n\r\nThere was a steep learning curve for image processing. My group struggled with it for several days, spending most of our time debugging and rerunning code that never seemed to work. We struggled to interpret the documentation of several OpenCV functions and encountered multiple problems with the custom message type. On Thursday we finally succeeded in getting the node to publish the correct data.\r\n\r\nThe blob detection node subscribed to the /camera/rgb/image_rect_color topic, which gave it access to the stream of images captured by the ZED camera. It published to the blob_info and image_echo topics. The custom blob_detect messages were sent over the blob_info topic. The Image topic was used to publish frames from the camera to which contours and points indicating the center of blobs were added. These altered images were used to debug the vision code and tweak color ranges. \r\n\r\n**Table 1: Color Ranges for Green and Red Blobs (Saturation and Value are Fractional)**\r\n\r\n| Color | Hue Min | Hue Max | Saturation Min | Saturation Max | Value Min | Value Max |\r\n|-------|:-------:|:--------:|:--------------:|:-------------:|:--------:|:----------:|\r\n|Red     |   0      |     15     |       .8         |       1     |    .7        |     1       |\r\n|Green    |   50      |   77      |       .4         |      1      |    .15        |    1        |\r\n\r\nThe vision node worked by creating two lists of contours: one for red blobs and one for green blobs. The 'official contour' was set to green by default. If the list of green contours was empty, the official contour would be set to red. The center and size of the blob was then determined using moments and the fields of the custom message type were filled out. Finally, the custom message type and the modified camera image were published.\r\n\r\n###Motion Node\r\n\r\nThe blob detection node took a lot longer to program than our group anticipated, so we did not get to spend as much time as we wanted on the motion code. To summarize, the node subscribed to the blob_info and /scan topics. It used the x term of the location tuple and compared it to the x coordinate at center of the camera's view to find the steering error. The error was input into a PID function to determine the steering angle. The size of the blob was used to tell the node when to switch between visual servoing and wall following. If the size of the blob made up more than 10% of the camera view, the node would make the change. \r\n\r\nThe wall following algorithm we implemented was similar to the one from the previous week; it took a range of points and used the minimum distance of these points as the distance from the wall. \r\n\r\nIt was necessary to add an additional motion command between visual servoing and wall follow. When the car reached the point that it was close enough to the blob to start wall following, it was perpendicular to the wall. This means that when the car attempted to wall follow in either direction, the LiDAR would detect the smallest distance to be extremely large, so the car would turn hard in that direction and drive in a circle indefinitely. To combat this, we added a manual steering command that would tell the car to steer full right for green and full right for red for a period of .75 seconds. That way, when the car finally switched to wall following, there would actually be a wall for it to detect using the LiDAR.\r\n\r\n###Testing\r\n\r\nWe did not get to test until the final day. Before testing the robot on the floor, we put it on a box so we could see what the wheels were doing without it actually moving. We then held a green blob in front of the camera and moved it around to see if the car was successfully steering towards the blob. Once we were sure this worked, we were ready to test it on the floor. \r\n\r\nTo make sure it was making it into the visual servo box, we added temporary code to made the robot stop right before the transition from visual servoing to wall following. This allowed us to tweak the desired area for the blob until the car stopped right inside the box. \r\n\r\nWe only got to test the entire system a couple of times before the weekly challenge. There were problems with sign mix ups; it was turning left on green or not following the correct wall. We thought that we had fixed this as we did have one successful test run with a green blob, but would have liked to test it many more times to confirm that they were correct. \r\n\r\n![WeeklyChallenge](https://cloud.githubusercontent.com/assets/18174572/17713920/5cdf9df0-63cb-11e6-810f-9680f78509df.png)\r\n\r\nFigure 6: Diagram of Week 2 Challenge (14)\r\n\r\n##Results\r\n\r\nFor week two's challenge, each car had three opportunities to complete the tasks. For each of these three tries, the blob color would be randomly chosen as would the starting orientation of the car. The blob would be either red or green and the car would either start pointing directly at the blob or be slightly rotated to the left or right. The time (if there was one) would be recorded for each run and the single best time would be used to rank the cars at the end.\r\n\r\nFor our first run, our car was assigned the green blob and was rotated slightly to the right. For the second run, we again were given the green blob but our car was not rotated. In the final run we had to detect a red blob and we started rotated to the right again. We were one of the seven teams that did not complete any runs successfully. This was unsurprising considering we only had one successful test run, but disappointing nonetheless.\r\n\r\nDuring the first and third runs, our car made it into the visual servoing box and turned in the correct direction, but just continued to turn in a circle and did not make it to the finish line. The car's most successful attempt was its second (green no rotation), where it made it into the box, made the correct turn (right), wall followed the left wall for a little while, but about three meters away from the finish line suddenly turned right and collided with the wall on the other side. There are many possible reasons for the car's malfunctions. It could have been something as simple as a sign error in the left and right steer values or something that would take more time to debug, like a faulty turn command. Given more time our team may have made some very different choices, such as splitting the visual servoing and wall following into two separate nodes instead of combining them into one. This week was difficult, but groups were able to accomplish the first goal, gaining experience with image processing, which would help them in the weeks to follow.\r\n\r\n************************************************************************************************************************\r\n\r\n#Week Three: Technical Work\r\n\r\n##Goals\r\n\r\nWeek three had a slightly different format than weeks one and two. Unlike in the previous two weeks, most of the lectures were not relevant to the lab work. Because of this, the academic and challenge goals of the week are noticeably disjoint.\r\n\r\nThe lectures during week three focused on localization and mapping. Students also learned about several path planning algorithms. The goal was to understand common algorithms that are used to let a robot know where it is in its environment and can be used to develop routes to specific goals.\r\n\r\nThe weekly challenge did not required groups to implement any localization or mapping functionality. The cars had to drive around for two minutes in an enclosed pen, avoid hitting any obstacles, and identify as many different colored blobs as possible.\r\n\r\n##Approach\r\n\r\nAlthough students did not implement localization, mapping, or the SLAM algorithm during the program, they were an important part of the student's conceptual learning for the week. Potential fields were implemented by almost all teams and were relevant to the localization and mapping lectures of the week. \r\n\r\n###Localization\r\n\r\nLocalization is the process of determining the pose of a robot with respect to an environment it is given a representation of (1). The representation is defined with respect to some external reference frame, such as the robot's starting position or nearby walls, corners, or markings. (15)\r\n\r\nDead reckoning localization uses odometry to estimate the robot's pose with respect to the initial coordinate frame. Dead reckoning has multiple error sources, such as wheel slip, gear backlash, noise from encoders, and sensor or processor quantization errors that accumulate over time to  make the pose less than accurate. (15)\r\n\r\n![Dead Reckoning Errors](https://cloud.githubusercontent.com/assets/18174572/17782992/fc071c9a-6543-11e6-9f89-d846b68e9272.png)\r\n\r\nFigure 1: Accumulated error from dead-reckoning localization (15)\r\n\r\nLandmarks with known reference frame pose embedded in the environment can help cut down on these errors. Landmarks can be natural, such as a wall corner, or artificial, like a surveyor's mark. They also vary in which sensor is able to detect it. Triangulation can be used to estimate pose with respect to two landmarks. (15)\r\n\r\n###SLAM Algorithm\r\n\r\nThe Simultaneous Localization and Mapping Algorithm (SLAM) algorithm allows a robot with no external coordinate reference to use a series of proprioceptive and exteroceptive (internal and external) measurements taken as it moves through an unknown environment to create a map of said environment (15). The robot is also able to localize itself with respect to the map it generates.\r\n\r\nSLAM is also most effective when landmarks are used. Localization based on environment features minimizes robot pose uncertainty and thus the uncertainty of the map (15). Laser scans or camera data could both be used to create the map.\r\n\r\n###Potential Fields\r\n\r\nThe potential field method is a form of path planning algorithm. The key idea of potential field algorithms is that the robot travels through a space such that it is attracted to a goal region and repelled from any obstacles (16). This can be implemented using localization or as a reactive algorithm. The latter was most relevant to the student's goals for the week. \r\n\r\nVectors are an excellent way to execute a potential field algorithm. The LiDAR scanner, creates 1081 data points containing the distance between the car and the nearest obstacle in that direction. Each of these points can be interpreted as vectors directed at the car; the shorter the distance between the car and the obstacle, the stronger the force of the vector. This effectively repels the car away from any obstacles. However, in reactive planning, there is no 'goal region' as the robot has no idea of its pose with respect to the environment, so the robot is not being attracted to anything, only repelled. To fix this, a large driving charge can be placed directly behind the robot to move it forward. This method works well when the robot is just trying to explore and avoid obstacles as was the goal in week three's challenge. \r\n\r\nThe following formulas can be used to find the repulsive force of a single point: \r\n\r\n                            |Ê| = Alaser / r^2                                              (eq. 1)\r\n\r\nWhere Alaser is some constant charge and r is the distance of the car to the obstacle for that particular point. By using this equation for every point and summing the results, one can obtain the total repulsive force. \r\n\r\nA major advantage of the potential field algorithm is that when a car gets extremely close to an obstacle, the repelling charge from the obstacle overpowers the driving force which causes the car to back up. This prevents it from getting stuck.\r\n\r\n###Challenge Approach\r\n\r\nMy team decided to use two nodes for the weekly challenge. One would control the cars motion and implement a potential field algorithm. The other would detect and record images of blobs that the car's camera captured. These nodes were completely independent of each other and did not communicate at all. We decided to have two independent nodes because it was not necessary for them to communicate. This week, we focused on perfecting the simplest algorithms that could accomplish our goals. \r\n\r\nWe decide to write the motion control first, followed by the vision node. Since we had spent all of last week learning about image processing, we felt more confident in that part of the code and wanted to get the unfamiliar potential field done first. Like the first week, for the most part all members of the group worked through the process together with very little task delegation. Codeshare.io allowed us to all look at the code together without crowding around one computer and was a very useful tool during the week. \r\n\r\nOne decision we made before writing the motion node was to add a functionality to help the car get \"unstuck\" if it ever stopped moving completely. Although rare, sometimes the driving force and the repelling forces equalize, causing the car to stall. To fix this, we chose to add code that told the car to back up if it had not moved for more than two seconds.\r\n\r\nWe also had to decide how to approach saving images that the vision node captured. The main concern was capturing many images detecting the same blob. The camera publishes multiple images per second, and the same blob would probably be present in several images in a row. It is unnecessary to detect any blob more than one time, so our group had to decide how to prevent repeat recognition from happening. This could be done using odometry or time. We decided to use distance. After the car saved a picture, it would have to travel one meter before capturing another. \r\n\r\n\r\n##Process\r\n\r\n###Explorer Node\r\n\r\nThe potential field algorithm was surprisingly simple to implement. The first draft of the code took less than half an hour to write. Other than a few syntax errors, the code was functional right off the bat and just needed tweaking. \r\n\r\nThe node subscribed to the 'scan' topic to receive LiDAR data and published steering commands to the 'vesc/ackermann_cmd_mux/input/navigation' topic. For our propelling charge we settled on a value of 4, and the Alaser value was 0.005. These took a lot of testing to figure out. If the propelling charge was not large enough, the car would not move forward, but if it were too large in comparison to the Alaser value, then it would collide with obstacles because their repelling charges would be completely overwhelmed. \r\n\r\n###Blob Search\r\n\r\nThe blob detection node would look for contours of all four colors and determine the size of the largest of each color. If this size was larger than a predetermined threshold height (100 pixels), then this contour would 'count.' It would be outlined and labeled and a picture of it would be saved. Originally we planned to only detect the largest blob, but by checking blobs of each color against a threshold the car would be able to detect more than one blob at once. \r\n\r\nThe node subscribed to the '/camera/rgb/image_rect_color' topic to recieve images and the '/vesc/odom' topic to receive odometry information. It published to the '/image_echo' topic and the '/exploring_challenge' topic, which would publish a string that said what color blobs it detected. Both of thse were used for debugging. \r\n\r\nIn order to receive any points in the challenge, the camera image with contours and labels for blobs added had to be saved in the '/home/racecar/challenge_photos' directory. We did this using the OpenCV function cv2.imwrite and naming the image file based on the time at which it was taken.\r\n\r\nWe had many problems perfecting color bounds for the four different blobs. On the HSV scale, a hue of 0 to 15 is supposed to indicate the color red, but the only way we could get the algorithm to correctly detect red was using a range of 100 to 125. We realized this was inverted (as a hue value around 120 is supposed to be blue, not red), and soon after discovered the problem. The original image was in BGR format, not RGB, so when we used the OpenCV function cv2.cvtColor to switch from RGB to HSV, we were not giving the function the correct starting format. By the time we fixed this however, we had just an hour before the final challenge. Because of this we were very rushed when determining HSV ranges and they were not as accurate as we wanted. \r\n\r\n**Table 1: Color Bounds for Blob Detection**\r\n\r\n| Color | Hue Min | Hue Max | Saturation Min | Saturation Max | Value Min | Value Max |\r\n|-------|:-------:|:--------:|:--------------:|:-------------:|:--------:|:----------:|\r\n|Red    |    0     |    15      |     .85           |      1      |      .2      |     .9       |\r\n|Green    |    30     |   75       |      .5          |     1       |     .4       |     1       |\r\n|Yellow   |    0     |    180      |        .3        |     1       |     .745       |    1        |\r\n|Blue    |    100     |   125       |    .4            |      1      |    0        |     .5       |\r\n\r\nWe tested the vision blob by only running the blob detection node and having it publish to the /image_echo topic and putting different colored blobs in front of the camera to see if it was correctly detecting them.\r\n\r\n\r\n##Results\r\n\r\nIn the final challenge, cars were given two minutes to navigate a pen in which red, green, yellow, and blue blobs were randomly dispersed. For each blob the car successfully detected and recorded in an image file, it was awarded ten points. For each collision, two points were deducted. There were also special pink blobs with different images inside of them. If the car was able to successfully identify what the image was, it would be awarded five points.\r\n\r\nIn the final challenge, car 7, my group's car, detected six blobs (no special blobs) and had just one collision, which ended us with a total score of 58 points. My team received second place overall for this performance. \r\n\r\nThe lack of time put into perfecting color ranges was apparent in the images that the vision node produced. Though it correctly identified six blobs, it also detected many things that were not blobs to be blobs and occasionally misidentified a blob as a different color than it actually was.\r\n\r\n![sucess](https://cloud.githubusercontent.com/assets/18174572/17710382/00521852-63bb-11e6-83a6-6a10a6911656.png)\r\n\r\nFigure 2: Successful detection of blob (10 pts), original image\r\n\r\n![Failure](https://cloud.githubusercontent.com/assets/18174572/17710383/00560a02-63bb-11e6-9a59-b3463de206e9.png)\r\n\r\nFigure 3: Detection of background as a blob, original image\r\n\r\n![Different Failure](https://cloud.githubusercontent.com/assets/18174572/17710379/00388d74-63bb-11e6-8739-9a6b26836600.png)\r\n\r\nFigure BLANK 4: Correct detection of green blob and incorrect detection of red blob, original image\r\n\r\nFrom the pictures it was clear that our car could have been more successful at blob detection if we had set the speed to be slightly lower. The images were blurred, which most likely did nothing to help the image processing. The car definitely went faster than necessary as it made at least five full loops around the course in the allotted two minutes. Clearer images could have led to more accurately identified blobs.\r\n\r\n************************************************************************************************************************\r\n",
  "note": "Don't delete this file! It's used internally to help with page regeneration."
}