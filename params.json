{
  "name": "Technical Paper for MIT Beaver Works Summer Institute, Sadie Allen",
  "tagline": "sadieiq694.github.io",
  "body": "#**Project Overview**\r\n\r\nOver the past four weeks, forty-six students with a strong interest in STEM from all over the country have participated in the pilot MIT Beaver Works Mini Grand Prix Challenge. Teams learned the basics of the algorithms involved in autonomous driving and used them to program RACECARs (Rapid Autonomous Complex-Environment Competing Ackermann-steering Robots). On August 5 the program culminated in a final competition to demonstrate everything that students learned.\r\n\r\n2016 is the pilot year of this program, created in large part by Dr. Bob Shin of Lincoln Laboratories. He saw a need for an engineering and robotics summer program, and Beaver Works Summer Institute (BWSI) was born. The program is based upon the MIT class 6.141, RSS (Robotic Science and Systems), and essentially compresses the material of that course into an intensive month long period. The amount of information covered made for a rigorous pace and exciting program for all involved.\r\n\r\nMonday through Friday, program participants would work in the Daniel Guggenheim Aeronautical Laboratory (MIT Building 33) from 9:00am to 5:00pm. Most mornings started with a technical lecture to prepare students for the lab that immediately followed. Every day right before lunch there was also a guest seminar speaker, usually a STEM professional. On Tuesdays and Thursdays, after lunch, Ms. Jane Connor would give a lecture on communication and teamwork. The rest of the day was devoted to more lab time during which students worked in groups of five to six, writing and testing code for the RACECARs.\r\n\r\nThe focus of the first week was on learning the basics of ROS, Linux, and Python, and taking the first steps in applying these skills through basic movement in our RACECARS. Students also learned about control systems that could be used to make the cars accomplish specific goals. In the final challenge of the week, cars raced side by side, following either the left or right wall, proceeding as fast and efficiently as possible.\r\n\r\nThe content during the second week primarily concerned “blob detection,” or, recognizing different colored sheets of paper and making decisions based on the color, shape, or size of the blob. Groups utilized the RACECAR’s ZED camera to capture images and used open CV to manipulate them. The week finished with a difficult challenge in which cars had to detect a red or green blob, drive towards it until they were a certain distance away, then, based on the color of the blob, turn left or right and \"wall follow.\"\r\n\r\nThe third week was originally supposed to cover localization and mapping, but, due to problems with software plans changed slightly. The technical lectures still covered those topics, but the labs focused on more reactive navigation; we built upon what we learned in the previous two weeks and programmed our cars to drive around a pen and avoid obstacles without use of a map, while simultaneously detecting different colored blobs. In the final challenge, the RACECAR’s had to drive around autonomously for two minutes and detect as many red, green, yellow, and blue blobs as possible, while avoiding any collisions.\r\n\r\nFinally, in week four the teams prepared for the exhibition day and grand prix race. Teams had to showcase code that they had written each week. Race day consisted of two technical challenges; the first was a “move and explore challenge” similar to week three’s final task, and the second was “make the correct turn,” similar to week two’s task. In addition to these challenges, the cars had to complete three time trials, one heat race against two other cars, and a final race with all nine cars on the grand prix race track.\r\n\r\nBy the end of the program, students gained confidence in ROS, Python, and Linux. They learned about and implemented many algorithms that are used by companies like Tesla and Google to program real autonomous vehicles. In addition, participants gained valuable robotics experience and learned about how to be successful when working in groups. The seminars from experts in various STEM fields showed students the many directions one can go if he or she is interested in engineering and technology, and it was truly an amazing opportunity to get to hear such varied experts speak. The MIT Beaver Works Summer Institute was an incredible opportunity that greatly benefited all who had the privilege to attend.\r\n\r\n***********************\r\n\r\n#**Week One: Technical Work**\r\n\r\n##**Goals**\r\n\r\n  The first week of the program served as a period to set the foundation of ROS, Linux, and Python\r\n  knowledge, as well as acquaint students with the RACECAR platform. Everyone started at a slightly\r\n  different level (for example, some students had prior Python experience, and not everyone completed the same amount of the pre-program coursework). For the first few days of week one, the instructors were\r\n  primarily concerned with getting all students on a level that would allow them to succeed in the\r\n  weeks to follow.\r\n\r\n  In week one groups were also given the task of programming their RACECARs to wall follow -- or,\r\n  drive forward and stay a specific distance away from a wall -- for approximately 30 feet as quickly as possible.\r\n  The cars had to be able to follow both the left and right walls and switch between the two on\r\n  command. Since all of the cars had the same speed cap (2 meters per second), the only way\r\n  for teams to differentiate themselves was by having a better algorithm (in terms of computation\r\n  time and oscillation levels).\r\n\r\n##**Approach**\r\n\r\n###**The RACECAR**\r\n\r\nIn order for groups to control their RACECARs, it was first crucial for them to understand them and all of their components.\r\n![RACECAR](https://cloud.githubusercontent.com/assets/18174572/17645837/ae61aabe-617e-11e6-96b2-f528a82376e1.png)\r\nFigure 1: Labeled illustration of the RACECAR used in this course (1); note that the cars used in the Beaver Works Summer Program did not have active stereo cameras, and the passive cameras were mounted above the LiDAR where the active camera appears in this image.\r\n\r\nThe robots used in this program were equipped with an advanced supercomputer and sensors that provided the data necessary for the algorithms to work. (1)\r\n  * **Chassis:** The basic frame of the vehicle is the Traxxas Rally 74076, a 1/10 scale RC car with four wheel drive and    Ackermann front wheel steering. It is capable of speeds up to 40 miles per hour, but for BWSI's purposes this was limited to 2 m/s (or 4.5 mph).\r\n  * **Processor:** The computer aboard the car is the Nvidia Jetson TX1 embedded systems module, running Ubuntu for ARM. It is equipped with 256 CUDA cores that deliver over 1 TeraFLOPs of performance (2). The large number of GPU cores allows the TX1 to perform many parallel operations at the same time, making it extremely fast and efficient. The speed definitely comes in handy for the RACECAR's purposes; data could be processed and commands could be given in real time, which is very important for the reactive navigation that the cars did for the vast majority of the time.\r\n  * **2D LiDAR:** Each car also had a 2D Hokuyo UST 10LX scanning laser rangefinder. The LiDAR has a range of .06m to 10m and an accuracy of +/-40mm. It has a scan angle of 270 degrees and a speed of 40Hz. The laser scanner was one of the most frequently used scanners throughout the duration of the program. It scans 270 degrees and splits it into 1081 points, calculates the distance each of the points is from the car, and returns the distances as a list. Specifically, it does this by sending out beams of light (lasers) and measuring the time it takes for them to come back. The following formula:\r\n\r\n                                    D = ct/2                                                    (eq. 1)\r\n\r\n* **2D LiDAR (cont):** Where c = speed of light and t = time, can then be used to determine the distance (D). This data was vital as most steering decisions the car made (all of them during week one) were based upon it.\r\n\r\n![Hokuyo](https://cloud.githubusercontent.com/assets/18174572/17646411/ba69fbc4-6195-11e6-8ebb-77e65e6b56e5.png)\r\n\r\nFigure 2: Hokuyo LiDAR and diagram depicting its range (1)\r\n  * **IMU:** A Sparkfun 9 Degrees of Freedom \"Razor\" Inertial Measurment Unit (IMU) was also present on each vehicle. The IMU has three sensors: a MEMS (MicroElectroMechanical System) three-axis accelerometer that assesses translational acceleration, a MEMS three-axis gyroscope that measures Coriolis force, and a three-axis Anisotropic MagnetoResisteance (AMR) magnetometer. The IMU is used to compute the current acceleration of the car.\r\n  * **Passive Stereo Camera:** Unlike the diagram pictured above, the cars utilized during the Beaver Works program had only one camera: a Sensorlabs ZED Passive Stereo Camera with automatic depth perception from .7 to 20 meters and a field of view of 110 degrees. Although groups never utilized the depth perception, this functionality of the camera is made possible because there are actually two separate cameras that record the same image, use image processing software to identify matching points in the images, and then solve the translation between them to obtain depth data. The farther apart an object seems to the two cameras, the closer it is.\r\n\r\n![Car and Controller](https://cloud.githubusercontent.com/assets/18174572/17651127/cbe4150e-622d-11e6-96c7-6885b09627ee.png)\r\n\r\nFigure 3: RACECAR and remote controller (1)\r\n\r\nIn addition, cars came with a controller (see above) that could be used to command the car's movements. Before ever running a program, the controller would be connected to the car. The left trigger worked as a \"deadman switch.\" Pressing it immediately stopped the car. This was a safety measure taken to protect the RACECARs. A human always controlled and could stop it car from colliding with an obstacle if a program did not run as planned.\r\n\r\n###**Basics of ROS**\r\n\r\nStudents also had to have a basic understanding of the Robot Operating System (ROS) in order to complete any of the assigned tasks.\r\n\r\nROS is an operating system that is used for robots. The overarching goal of the system is to support code reuse in robotics; before ROS, people had to start from scratch whenever they wanted to program a new robot, even though a lot of the code that they wrote had already written before (4). Because ROS is open source, meaning all source code is made available, ROS saves software developers a huge amount of time because they can reuse code written by others. This allows progress in robotics to proceed more rapidly. ROS is a powerful, flexible (supports multiple languages and multi-machine systems) tool that is widely used in academia and industry and is supported by the Open Source Robotics Foundation (OSRF). (5)\r\n\r\nOne aspect of ROS that makes it so effective for robotics software development is how its design supports modularity. Most tasks that robots have to accomplish are complex and have many different components. Completing each component linearly is time consuming and does not take advantage of the Jetson TX1's many GPU cores. Luckily, ROS makes it very simple to employ a modular software design by allowing elaborate tasks to be broken into simpler pieces. Then, multiple parts of the problem can be solved at the same time, allowing programs to run much more quickly. (5)\r\n\r\nROS utilizes modularity by organizing programs into nodes. Nodes are processes performing specific computations in the ROS system. There can be as few as one, and as many as one hundred, in a single program. The nodes have to be able to communicate with each other, and they do that using messages and topics. Messages are strictly typed packets of data sent between ROS nodes. They are sent over topics: unidirectional communication links between ROS nodes. When a node wants to send data, it must **publish** a message to a specific topic, and the node that wants to receive that data must **subscribe** to that topic. Each topic is linked to a single message type. It is important to note that one or more nodes may publish messages to a topic and one or more nodes can subscribe to any topic. In addition, an individual node can publish or subscribe to as many topics as it needs to. The ROS Master process is responsible for linking nodes together. It coordinates the peer-to-peer communication links between nodes. Nodes that wish to publish advertise to the ROS Master on the desired topic. Nodes that wish to subscribe to that topic tell the ROS Master, and it establishes a connection between the two nodes. (5)\r\n\r\n\r\n###**Control Systems**\r\nTo help us with this first challenge, a robotics software engineer from NASA's Jet Propulsion Laboratory (JPL), Kyle Edelberg, gave technical lectures on control systems.\r\n\r\nA simple definition of a control system is something that takes a system (specifically the RACECAR for the program's purposes) from state A to state B, the desired state. The controller is implemented in the software (the program(s) that tell the system what to do), and the system is the hardware, the RACECAR for Beaver Works' purposes. (6)\r\n\r\nThere are two basic types of control that a system can use: open loop control and closed loop control. (6)\r\n\r\nOpen loop control is the more primitive of the two types of control system. The controller tells the system to do something and receives no feedback to let it know how close it is to completing the task. An example of open loop control is if a RACECAR had to move 10m, and the top speed was 2m/s, so the program told the car to run at full speed forward for five seconds. The car would run for the five seconds and stop, but the controller would have no idea if it was successful. In addition, this method does not account for many errors, such as the wheels sticking, slipping, or being misaligned; therefore, if any of that happened, the program would not be able to adjust and achieve the desired state. Open loop control is used when the communication between the controller and the system would be too slow to effectively help decision making. (6)\r\n\r\nClosed loop control is usually preferred over open loop. In this control method, the controller receives feedback from the system to help it achieve the desired value. Groups used closed loop control for their wall following algorithms. (6)\r\n\r\n![Open Loop Control (Wall Follow)](https://cloud.githubusercontent.com/assets/18174572/17651717/ff97b6e6-623a-11e6-8448-2cbcaff08204.png)\r\n\r\nFigure 3: Diagram depicting closed loop control for wall following (6)\r\n\r\nThe above diagram illustrates how closed loop control works for wall following. The programmer inputs a desired distance (Ddes) into the controller and the current distance(D) from the wall is found with the LiDAR. Both of the distances are then sent to the controller. The controller subtracts the current distance from the desired distance to obtain the error. The controller sends steering commands to the system that drive this error as close to zero as possible. The system sends more data back to the controller in a continuous loop. (6)\r\n\r\nMr. Edelburg introduced two types of closed loop control that could be used: bang bang control and Proportional Integral Derivative (PID) control.\r\n\r\nBang bang control is the simplest form of closed loop control. A bang bang algorithm looks at the error value (Ddes - D). If it's positive, the car is too close to the wall, so the controller tells the car to steer as far as possible away from the wall (e.g., if following the right wall, the steer command would be -1, which is a full left turn). If the error value is negative, the car is too far away from the wall so the program tells the car to steer all the way towards the wall (again, in a right wall following scenario the command would be 1, full right). Bang bang control is effective, but it causes significant oscillations in the car's forward motion. oscillations lead to the car traveling farther than if it were able to drive straight, which is not ideal when teams wanted their cars to move as quickly as possible. (6)\r\n\r\nPID control is another fairly simple control scheme that makes use of the following equation (6):\r\n\r\n                            u = kp*e + ki*∫e + kd*ẻ                                               (eq. 2)\r\n\r\nWhere u is the steering command (a number between -1 and 1, or, full left and full right), kp, ki, and kd are experimentally determined constants, e is the error, ∫e is the summation of the errors (integral with respect to time), and ẻ is the derivative of the errors. Each of the terms of this equation has a specific goal. The kp term initially brings the error value close to zero, the ki term drives it all the way down, and the kd value helps with stability, i.e., minimizes oscillations (6). Most groups opted to use PID control in week one's final challenge.\r\n\r\n![PID](https://camo.githubusercontent.com/bbede27c5fa69f4764cf2727cb42740aa7d46b5b/68747470733a2f2f75706c6f61642e77696b696d656469612e6f72672f77696b6970656469612f636f6d6d6f6e732f332f33332f5049445f436f6d70656e736174696f6e5f416e696d617465642e676966)\r\n\r\nFigure 4: Animation showing the effects of changing PID values (3)\r\n\r\n###**Challenge Approach**\r\n\r\nAfter gaining the necessary background through lectures and short labs, groups were ready to start working on the implementation of a wall following node. When deciding our approach, my team prioritized learning and obtaining experience with the robot above all else, which is reflected in all of the choices that were made.\r\n\r\nIn order to get more practice writing nodes, my group decided to first write one that used the bang bang control system and then write a PID node. Although this approach left less time to tweak PID values, my team decided it was worth it in the long run to have a better understanding of nodes and ROS in a general sense.\r\n\r\nAnother decision our group made going into week one's challenge was to not delegate tasks; for the most part we all worked on everything together. Again, working as a full group was not the most efficient way to go about the task, but it made it much easier to ensure that all group members understood what was going on throughout the whole process.\r\n\r\nThe final decision the group had to make before starting to code was how to interpret the laser data. This at first seems like a straightforward problem; find the point on the scan that is at a 90 degree angle with the front of the car and use that distance as the distance from the wall. However, with the car steering and very rarely driving exactly parallel to the wall, this would not always be the correct distance. The car would sometimes be much closer to the wall than that single point would appear. To combat this there were two options. The first was to use trigonometry by taking two points at a 30 degree angle and using the law of sines and the law of cosines to solve for the actually distance from the wall. The second was to take a large swathe of points and find the one with the smallest value. This value would be the minimum distance between the car and the wall. Our group chose to go with the second approach because it was simpler to program and did not make use of computationally expensive trigonometric functions. In addition it was more reliable than depending on just two points. If the car reached a corner, the point farther out in front could be an extremely high value that would cause the calculated distance to be incorrect. By looking at the smallest value within a range, the chance of being thrown by massive outliers is virtually eliminated.\r\n\r\n##**Process**\r\n\r\nThe process of implementing a wall follow algorithm was cyclical. Our team would write code, test it to see what it would do, change the code, and retest. To get started we used node templates found on the ROS wiki. This gave us a good idea of the format we needed to use and then we just filled it in with our own code. There were a few bumps along the way as this was our first time writing a node but we worked methodically through everything and made sure to thoroughly comment the code to make it understandable.\r\n\r\nThe bang bang controller followed solely the left wall and had a scan range of 540 to 930. The node 'wall_bang' subscribed to the '/scan' topic to obtain the LiDAR data and published drive commands to the '/vesc/ackermann_cmd_mux/input/navigation' topic. Once the code was functional, it required very little testing because there were no values to tweak. The one thing we added on top of basic bang bang control was a threshold value; if the error was small enough (specifically, if the absolute value of the error was less than .03 meters), the node will publish a steering command of zero, which tells the car to go straight. The bang bang controller was functional but oscillated a great deal.\r\n\r\nAfter successfully coding the bang bang controller, we moved on to PID. One thing we decided during the coding process was to turn PID control into PD control; the integral term just serves to bring the error value closer to zero, and in our case, that was not important. The car just had to follow a wall as quickly as possible; the exact distance it was from the wall, which is the only thing that implementing the I portion of PID control would change, did not matter. By scratching it we saved computational time and had more time to test PD values.\r\n\r\nWriting the PD node did not take that long because we just had to alter and add a few things in the bang bang algorithm.  We completed the process in steps. First, we had only a a P term. Then, once we had tested that and made sure that part was functional, we added in the derivative element. Breaking the code down into testable parts makes the debugging process much more efficient because the sources of errors are easier to locate. Of course, once the D section was added in, the kp value had to be altered. Our group spent a lot of our lab time testing different kp and kd values to optimize the algorithm. On Kyle Edelberg's recommendation, we started with the kp value very low and gradually incremented it, then did the same with kd. The values we settled on were .7 for kp and .6 for kd.\r\n\r\nThis algorithm could follow either the left or the right wall depending on what button was pressed on the controller. The 'a' button made it follow the left wall and 'b' made it follow the right. If no buttons were pressed, it would default to following the right wall. For our scan ranges, we used 200 to 540 to for the right and 540 to 900 for the left.\r\n\r\nAnother feature we added to this node was a 'safety.' We took a small scan directly in front of the robot (from 525 to 555), and if the minimum distance to a point within that range was less than .5 meters, the speed of the robot would be set to -.1. We used -.1 instead of 0 to make the robot stop more quickly. Although the safety could have been implemented in its own node, it seemed simpler to us to just make it part of the wall following node to avoid dealing  with node hierarchy.\r\n\r\nFull code for both algorithms can be found in the scripts folder of this github page.\r\n\r\n##**Results**\r\n\r\n  The final challenge was set up as a \"drag race.\" In the first round, cars had to complete two time\r\n  trials: one following the left wall and one following the right. Each car's fastest time was used\r\n  to place them in the second round. The fastest car got a bye to the next round, and the rest of the\r\n  nine cars were paired off and raced head to head. The races were best two out of three and the\r\n  fastest of each of these pairings moved on to the final round, as well as the fastest losing car\r\n  from all the rounds. In the final round three pairs of cars again raced best two out of three, this\r\n  time vying for gold, silver, and bronze medals.\r\n\r\n  In the time trials, car 45, my group's car, came in fourth with a time of 8.645 seconds. In round\r\n  two, we were up against car 34, which did not complete any successful runs in the three attempts,\r\n  so we advanced to the final round. In the final round the start was slightly different from in the\r\n  time trials; in the time trials, the timer would start as soon as the car moved across the start\r\n  line, but in the finals it would start as soon as the start signal was given. The cars take time\r\n  to get started, so this change was reflected in the final results. Our best time was 9.58 seconds,\r\n  almost a full second slower than in the time trials. We placed sixth overall in the final round.\r\n\r\n*************************\r\n\r\n#**Week Two: Technical Work**\r\n\r\n##**Goals**\r\n\r\nAfter week one, students felt more comfortable with the cars and basic motion commands. So, for week two, groups built upon these skills by combining them with basic computer vision.\r\n\r\nSimilar to the previous week, there were two main goals to accomplish. The first was to  learn about image processing and become comfortable using OpenCV, an open source computer vision software library (7). The final challenge goal incorporated things learned during week one and week two. The cars had to be able to detect a brightly colored piece of construction paper, or, 'blob', and drive toward it using visual servoing. This blob was either red or green. Once the robot got a certain distance away from it (approximately one meter), it had to make a steering decision based upon this color. If the blob was green, the car had to turn right and follow the left wall to a finish line. If it was red, the car had to turn left and follow the right wall to a different finish line. Teams were to complete these tasks as quickly as possible during the final challenge.\r\n\r\n##**Approach**\r\n\r\nAgain, some background knowledge is necessary before my group's approach can be described.\r\n\r\n###**Computer Representation of Images**\r\n\r\nA computer deals with color images as 2D pixel arrays where each pixel encodes a color (8). There are several ways in which computers can represent colors, but in the Beaver Works course students mostly worked with RGB and HSV formats.\r\n\r\nThe human eye perceives color using specialized cells called cones. There are three different types of cones in our eyes that all respond most strongly to different wavelengths of light; red, green, or blue (9). RGB representation is based off the human visual system. Each pixel in the 2D array that represents an image encodes a color with the triplet (R, G, B), indicating the amount of each color of light in that pixel. Because color pixels are typically encoded in 24 bits, or, 8 bits per color, the R, G, and B components take values between 0 and 255 (8). (0, 0, 0) encodes black, (255, 255, 255) encodes white, and any triplet in which all three values are the same represents gray. (255, 0, 0) would be a 'perfect' red.\r\n\r\nAn alternative to RGB is the HSV color model. This system also uses triplets to represent colors, but the three values represent hue, saturation, and value. Hue is out of 180 degrees. 0 degrees is red, 60 degrees is green, and 120 degrees is blue. Saturation is out of 255 and represents the intensity of the color. Value is also on a scale of 0 to 255 and is a measure of the brightness of a color (8).\r\n\r\n![HSV Chart] (https://cloud.githubusercontent.com/assets/18174572/17703787/f45c870e-63a0-11e6-86b2-d12b2cd06902.png)\r\n\r\nFigure 1: Diagram showing how colors change in accordance with HSV values (10)\r\n\r\nThe HSV model works better for blob detection because it is much more robust under illumination changes.\r\n\r\n###**Intro To OpenCV Tools**\r\n\r\nStudents used OpenCV to process the images received from the ZED camera. Specific tools in the library that were particularly useful for week two's challenge were:\r\n\r\n* **Creating A Mask**: Using cv2.inRange, one can select upper and lower bounds for color values and create a binary \"mask\" that highlights a specific color. This method can be used to initially identify blobs of specific colors. (7)\r\n\r\n![mask](https://cloud.githubusercontent.com/assets/18174572/17709879/deab533c-63b8-11e6-95a7-1b490beab197.png)\r\n\r\nFigure 2: Side by side of original image and binary mask for red range (11)\r\n\r\n* **Creating a Contour**: Contours are essentially curves that join all continuous points along a boundary having the same color. They work best on binary images, which is why a mask is often utilized to isolate the specific colors. By using the function cv2.findContours, one can identify contours using the binary image. The contours can then be applied to the original image using cv2.drawContours (7).\r\n\r\n![contour](https://cloud.githubusercontent.com/assets/18174572/17710445/4aed708c-63bb-11e6-83fa-fa6570613979.png)\r\n\r\nFigure 3: Image with contours applied (11)\r\n\r\n* **Using Moments**: Moments (cv2.moments) can be used to find the center of contours, which is useful to know when visual servoing (7). One can compare the x coordinate of the center of a contour (that outlines the desired blob) to the x coordinate of the center of the camera's frame and make steering decisions based on the difference.\r\n\r\n![center](https://cloud.githubusercontent.com/assets/18174572/17711449/ac908e7e-63bf-11e6-8275-5bdbf1ecbd89.png)\r\n\r\nFigure 4: Image with center identified (11)\r\n\r\n* **Creating a Bounding Rectangle**: cv2.boundingrect, as the name suggests, draws a rectangle around a contour. It is also one of several functions that can be used to determine the rough area of a blob, as it computes both height and width (7). However, this approach to finding the area is only accurate when the blob is rectangular in shape.\r\n\r\n![bounding rectangle](https://cloud.githubusercontent.com/assets/18174572/17712492/28f3e264-63c4-11e6-9bed-37eb7119b0af.png)\r\n\r\nFigure 5: Side by side view of red contour and bounding rectangle based off of that contour (12)\r\n\r\nThese tools were primarily what groups used to work with blobs for week two's challenge.\r\n\r\n###**Custom Messages**\r\n\r\nCustom messages were another important tool introduced to students. Sometimes it is necessary to communicate multiple pieces of information between nodes of varying different types (13). This is made simple by custom message types. An example is as follows:\r\n\r\n      Header header\r\n      std_msgs/String color\r\n      std_msgs/Float64 size\r\n      geometry_msgs/Point location\r\n\r\nThis is the message type my group used in the week two challenge to communicate information about the blob. The custom message allowed the color, size, and location of the blob to be sent all together instead of individually.\r\n\r\n###**Challenge Approach**\r\n\r\nWe decided to handle the challenge tasks with two nodes. One would handle vision and blob detection and one would handle motion commands. By writing fewer nodes, we hoped to save time that we could spend working with image processing, which proved to be difficult in the preliminary labs.\r\n\r\nThe blob detection node would determine the size, location, and color of the blob and pass it to the motion control node, which would use that information to make all driving decisions.\r\n\r\nIn terms of task delegation, our group split so some of us were working on blob detection and others worked on motion commands. Again, time played a big role in this decision. It would have been preferable for the entire group to work through everything together to ensure unanimous comprehension of the code, but because of time constraints we went with a \"divide and conquer\" approach.\r\n\r\n##**Process**\r\n\r\n###**Blob Detection Node**\r\n\r\nThere was a steep learning curve for image processing. My group struggled with it for several days, spending most of our time debugging and rerunning code that never seemed to work. We struggled to interpret the documentation of several OpenCV functions and encountered multiple problems with the custom message type. On Thursday we finally succeeded in getting the node to publish the correct data.\r\n\r\nThe blob detection node subscribed to the ''/camera/rgb/image_rect_color' topic, which gave it access to the stream of images captured by the ZED camera. It published to the 'blob_info' and 'image_echo' topics. The custom 'blob_detect' messages were sent over the 'blob_info' topic. The Image topic was used to publish frames from the camera to which contours and points indicating the center of blobs were added. These altered images were used to debug the vision code and tweak color ranges.\r\n\r\n**Table 1: Color Ranges for Green and Red Blobs (Saturation and Value are Fractional)**\r\n\r\n| Color | Hue Min | Hue Max | Saturation Min | Saturation Max | Value Min | Value Max |\r\n|-------|:-------:|:--------:|:--------------:|:-------------:|:--------:|:----------:|\r\n|Red     |   0      |     15     |       .8         |       1     |    .7        |     1       |\r\n|Green    |   50      |   77      |       .4         |      1      |    .15        |    1        |\r\n\r\nThe vision node worked by creating two lists of contours: one for red blobs and one for green blobs. The 'official contour' was set to green by default. If the list of green contours was empty, the official contour would be set to red. The center and size of the blob was then determined using moments and the fields of the custom message type were filled out. Finally, the custom message type and the modified camera image were published.\r\n\r\n###**Motion Node**\r\n\r\nThe blob detection node took a lot longer to program than our group anticipated, so we did not get to spend as much time as we wanted to on the motion code. To summarize, the node subscribed to the 'blob_info' and ''/scan' topics. It used the x term of the location tuple and compared it to the x coordinate at center of the camera's view to find the steering error. The error was input into a PID function to determine the steering angle. The size of the blob was used to tell the node when to switch between visual servoing and wall following. If the size of the blob made up more than 10% of the camera view, the node would make the change.\r\n\r\nThe wall following algorithm we implemented was similar to the one from the previous week; it took a range of points and used the minimum distance of these points as the distance from the wall.\r\n\r\nIt was necessary to add an additional motion command between visual servoing and wall follow. When the car reached the point that it was close enough to the blob to start wall following, it was perpendicular to the wall. This means that when the car attempted to wall follow in either direction, the LiDAR would detect the smallest distance to be extremely large, so the car would turn hard in that direction and drive in a circle indefinitely. To combat this, we added a manual steering command that would tell the car to steer full right for green and full left for red for a period of .75 seconds. That way, when the car finally switched to wall following, there would actually be a wall for it to detect using the LiDAR.\r\n\r\n###**Testing**\r\n\r\nWe did not get to test until the final day. Before testing the robot on the floor, we put it on a box so we could see what the wheels were doing without it actually moving. We then held a green blob in front of the camera and moved it around to see if the car was successfully steering towards the blob. Once we were sure this worked, we were ready to test on the floor.\r\n\r\nTo make sure it was making it into the visual servo box, we added temporary code to made the robot stop right before the transition from visual servoing to wall following. This allowed us to tweak the desired area for the blob until the car stopped right inside the box.\r\n\r\nWe only got to test the entire system a couple of times before the weekly challenge. There were problems with sign mix ups; the car was turning left on green and later it was not following the correct wall. We thought that we had fixed these problems as we did have one successful test run with a green blob, but would have liked to test it many more times to confirm that they were correct.\r\n\r\n![WeeklyChallenge](https://cloud.githubusercontent.com/assets/18174572/17713920/5cdf9df0-63cb-11e6-810f-9680f78509df.png)\r\n\r\nFigure 6: Diagram of Week 2 Challenge (14)\r\n\r\n##**Results**\r\n\r\nFor week two's challenge, each car had three opportunities to complete the tasks. For each of these three tries, the blob color would be randomly chosen as would the starting orientation of the car. The blob would be either red or green and the car would either start pointing directly at the blob or be slightly rotated to the left or right. The time (if there was one) would be recorded for each run and the single best time would be used to rank the cars at the end.\r\n\r\nFor our first run, our car was assigned the green blob and was rotated slightly to the right. For the second run, we again were given the green blob but our car was not rotated. In the final run we had to detect a red blob and we started rotated to the right again. We were one of the seven teams that did not complete any runs successfully. This was unsurprising considering we only had one successful test run, but disappointing nonetheless.\r\n\r\nDuring the first and third runs, our car made it into the visual servoing box and turned in the correct direction, but just continued to turn in a circle and did not make it to the finish line. The car's most successful attempt was its second (green no rotation), where it made it into the box, made the correct turn (right), wall followed the left wall for a little while, but about three meters away from the finish line suddenly turned right and collided with the wall on the other side. There are many possible reasons for the car's malfunctions. It could have been something as simple as a sign error in the left and right steer values or something that would take more time to debug, like a faulty turn command. Given more time our team may have made some very different choices, such as splitting the visual servoing and wall following into two separate nodes instead of combining them into one. This week was difficult, but groups were able to accomplish the first goal, gaining experience with image processing, which would help them in the weeks to follow.\r\n\r\n***************************\r\n\r\n#**Week Three: Technical Work**\r\n\r\n##**Goals**\r\n\r\nWeek three had a slightly different format than weeks one and two. Unlike in the previous two weeks, most of the lectures were not relevant to the lab work. Because of this, the academic and challenge goals of the week are noticeably disjoint.\r\n\r\nThe lectures during week three focused on localization and mapping. Students also learned about several path planning algorithms. The goal was to understand common algorithms that are used to let a robot know where it is in its environment and ones that can be used to develop routes to specific goals.\r\n\r\nThe weekly challenge did not require groups to implement any localization or mapping functionality. The cars had to drive around for two minutes in an enclosed pen, avoid hitting any obstacles, and identify as many different colored blobs as possible.\r\n\r\n##**Approach**\r\n\r\nAlthough students did not implement localization, mapping, or the SLAM algorithm during the program, they were an important part of the student's conceptual learning for the week. Potential fields were implemented by almost all teams and were relevant to the localization and mapping lectures of the week.\r\n\r\n###**Localization**\r\n\r\nLocalization is the process of determining the pose of a robot with respect to an environment of which it is given a representation (1). The representation is defined with respect to some external reference frame, such as the robot's starting position or nearby walls, corners, or markings. (15)\r\n\r\nDead reckoning localization uses odometry to estimate the robot's pose with respect to the initial coordinate frame. Dead reckoning has multiple error sources, such as wheel slip, gear backlash, noise from encoders, and sensor or processor quantization errors that accumulate over time to  make the pose less than accurate. (15)\r\n\r\n![Dead Reckoning Errors](https://cloud.githubusercontent.com/assets/18174572/17782992/fc071c9a-6543-11e6-9f89-d846b68e9272.png)\r\n\r\nFigure 1: Accumulated error from dead-reckoning localization (15)\r\n\r\nLandmarks with known reference frame positions that are embedded in the environment can help cut down on localization errors. Landmarks can be natural, such as a wall corner, or artificial, like a surveyor's mark. They also vary in which sensor is able to detect it. Triangulation can be used to estimate pose with respect to two landmarks. (15)\r\n\r\n###**SLAM Algorithm**\r\n\r\nThe Simultaneous Localization and Mapping (SLAM) algorithm allows a robot with no external coordinate reference to use a series of proprioceptive and exteroceptive (internal and external) measurements taken as it moves through an unknown environment to create a map(15). The robot is also able to localize itself with respect to the map it generates.\r\n\r\nSLAM is most effective when landmarks are used. Localization based on environment features minimizes robot pose uncertainty and thus the uncertainty of the map (15). Laser scans or camera data could both be used to create the map.\r\n\r\n###**Potential Fields**\r\n\r\nThe potential field method is a form of path planning algorithm. The key idea of potential field algorithms is that the robot travels through a space such that it is attracted to a goal region and repelled from any obstacles (16). This can be implemented using localization or as a reactive algorithm. The latter was most relevant to the student's goals for the week.\r\n\r\nVectors are an excellent way to execute a potential field algorithm. The LiDAR scanner, creates 1081 data points containing the distance between the car and the nearest obstacle in that direction. Each of these points can be interpreted as vectors directed at the car; the shorter the distance between the car and the obstacle, the stronger the force of the vector. This effectively repels the car away from any obstacles. However, in reactive planning, there is no 'goal region' as the robot has no idea of its pose with respect to the environment, so the robot is not being attracted to anything, only repelled. To fix this, a large propelling charge can be placed directly behind the robot to move it forward. This method works well when the robot is just trying to explore and avoid obstacles as was the goal in week three's challenge.\r\n\r\nThe following formulas can be used to find the repulsive force of a single point:\r\n\r\n                            |Ê| = Alaser / r^2                                              (eq. 1)\r\n\r\nWhere Alaser is some constant charge and r is the distance of the car to the obstacle for that particular point. By using this equation for every point and summing the results, one can obtain the total repulsive force.\r\n\r\nA major advantage of the potential field algorithm is that when a car gets extremely close to an obstacle, the repelling charge from the obstacle overpowers the driving force which causes the car to back up. This prevents it from getting stuck.\r\n\r\n###**Challenge Approach**\r\n\r\nThis week, we focused on perfecting the simplest algorithms that could accomplish our goals.\r\n\r\nMy team decided to use two nodes for the weekly challenge. One would control the cars motion and implement a potential field algorithm. The other would detect and record images of blobs that the car's camera captured. These nodes were completely independent of each other and did not communicate at all. We decided to have two independent nodes because it was not necessary for them to communicate.\r\n\r\nWe decide to write the motion control first, followed by the vision node. Since we had spent all of last week learning about image processing, we felt more confident in that part of the code and wanted to get the unfamiliar potential field algorithm done first. Like the first week, for the most part all members of the group worked through the process together with very little task delegation. Codeshare.io allowed us to look at the code together without crowding around one computer and was a very useful tool during the week.\r\n\r\nOne decision we made before writing the motion node was to add a functionality to help the car get \"unstuck\" if it ever stopped moving completely. Although rare, sometimes the driving force and the repelling forces equalize, causing the car to stall. To fix this, we chose to add code that told the car to back up if it had not moved for more than two seconds.\r\n\r\nWe also had to decide how to approach saving images that the vision node captured. The main concern was capturing many images detecting the same blob. The camera publishes multiple images per second, and the same blob would probably be present in several images in a row. It is unnecessary to detect any blob more than one time, so our group had to decide how to prevent repeat recognition from happening. This could be done using odometry or time. We decided to use distance. After the car saved a picture, it would have to travel one meter before capturing another.\r\n\r\n##**Process**\r\n\r\n###**Explorer Node**\r\n\r\nThe potential field algorithm was surprisingly simple to implement. The first draft of the code took less than half an hour to write. Other than a few syntax errors, the code was functional right off the bat and just needed tweaking.\r\n\r\nThe node subscribed to the 'scan' topic to receive LiDAR data and published steering commands to the 'vesc/ackermann_cmd_mux/input/navigation' topic. For our propelling charge we settled on a value of 4, and the Alaser value was 0.005. These took a lot of testing to figure out. If the propelling charge was not large enough, the car would not move forward, but if it were too large in comparison to the Alaser value, then it would collide with obstacles because their repelling charges would be completely overwhelmed.\r\n\r\n###**Blob Search**\r\n\r\nThe blob detection node would look for contours of all four colors and determine the size of the largest contour of each color. If this size was larger than a predetermined threshold height (100 pixels), then this contour would 'count.' It would be outlined and labeled and a picture of it would be saved. Originally we planned to only detect the largest blob, but checking blobs of each color against a threshold allowed the car to detect more than one blob at once.\r\n\r\nThe node subscribed to the '/camera/rgb/image_rect_color' topic to receive images and the '/vesc/odom' topic to receive odometry information. It published to the '/exploring_challenge' topic, which would publish a string that said what color blobs it detected, and the '/image_echo' topic. The only reason the node published to either of these topics was because it helped with debugging.\r\n\r\nIn order to receive any points in the challenge, the camera image with contours and labels for blobs added had to be saved in the '/home/racecar/challenge_photos' directory. We did this using the OpenCV function cv2.imwrite and naming the image file based on the time at which it was taken.\r\n\r\nWe had many problems perfecting color bounds for the four different blobs. On the HSV scale, a hue of 0 to 15 is supposed to indicate the color red, but the only way we could get the algorithm to correctly detect red was using a range of 100 to 125. We realized this was inverted (as a hue value around 120 is supposed to be blue, not red), and soon after discovered the problem. The original image was in BGR format, not RGB, so when we used the OpenCV function cv2.cvtColor to switch from RGB to HSV, we were not giving the function the correct starting format. By the time we fixed this however, we had just an hour before the final challenge., so we were very rushed when determining HSV ranges and they were not as accurate as we wanted.\r\n\r\n**Table 1: Color Bounds for Blob Detection**\r\n\r\n| Color | Hue Min | Hue Max | Saturation Min | Saturation Max | Value Min | Value Max |\r\n|-------|:-------:|:--------:|:--------------:|:-------------:|:--------:|:----------:|\r\n|Red    |    0     |    15      |     .85           |      1      |      .2      |     .9       |\r\n|Green    |    30     |   75       |      .5          |     1       |     .4       |     1       |\r\n|Yellow   |    0     |    180      |        .3        |     1       |     .745       |    1        |\r\n|Blue    |    100     |   125       |    .4            |      1      |    0        |     .5       |\r\n\r\nWe tested the color values by only running the blob detection node and having it publish to the /image_echo topic and putting different colored blobs in front of the camera to see if it was correctly identifying them.\r\n\r\n##**Results**\r\n\r\nIn the final challenge, cars were given two minutes to navigate a pen in which red, green, yellow, and blue blobs were randomly dispersed. For each blob the car successfully detected and recorded in an image file, it was awarded ten points. For each collision, two points were deducted. There were also special pink blobs with different images inside of them. If the car was able to successfully identify what the image was, it would be awarded five points.\r\n\r\nIn the final challenge, car 7, my group's car, detected six blobs (no special blobs) and had just one collision, which ended us with a total score of 58 points. My team received second place overall for this performance.\r\n\r\nThe lack of time put into perfecting color ranges was apparent in the images that the vision node produced. Though it correctly identified six blobs, it also detected many things that were not blobs to be blobs and occasionally misidentified a blob as a different color than it actually was.\r\n\r\n![sucess](https://cloud.githubusercontent.com/assets/18174572/17710382/00521852-63bb-11e6-83a6-6a10a6911656.png)\r\n\r\nFigure 2: Successful detection of blob (10 pts), original image\r\n\r\n![Failure](https://cloud.githubusercontent.com/assets/18174572/17710383/00560a02-63bb-11e6-9a59-b3463de206e9.png)\r\n\r\nFigure 3: Detection of background as a blob, original image\r\n\r\n![Different Failure](https://cloud.githubusercontent.com/assets/18174572/17710379/00388d74-63bb-11e6-8739-9a6b26836600.png)\r\n\r\nFigure BLANK 4: Correct detection of green blob and incorrect detection of red blob, original image\r\n\r\nFrom the pictures it was clear that our car could have been more successful at blob detection if we had set the speed to be slightly lower. The images were blurred, which most likely did nothing to help the image processing. The car definitely went faster than necessary as it made at least five full loops around the course in the allotted two minutes. Clearer images could have led to more accurately identified blobs.\r\n\r\n***************************\r\n\r\n#**Technical Work: Week Four**\r\n\r\n##**Goals**\r\n\r\nThe goal of the final week was for students to bring together everything they had learned in a showcase at Walker Memorial Hall on Friday, August 5. The exhibition consisted of three main sections: two technical challenges and the mini grand prix.\r\n\r\nThe first tech challenge was the same as the third week's challenge; Cars had to explore autonomously and avoid obstacles while recognizing and recording different colored blobs.\r\n\r\nThe second tech challenge was somewhat similar to week two's challenge. The cars were required to detect a blob and based on its color turn either left or right. This is different from the original week two challenge because the cars were not required to use visual servoing and did not have to wall follow. The challenge was altered in this way so that code from it could be reused for the mini grand prix (see details below).\r\n\r\nThe first part of the mini grand prix consisted of time trials (three attempts per car) with the cars alone on the track. On one of the attempts, a shortcut would be opened, signified by the red blob above the shortcut entrance being changed to green. If a car could successfully take the shortcut, it would have a significantly faster time, but if it took it when it was not open, the car would not receive a time for that run.\r\n\r\nAfter the time trials there were three heat races in which three cars would race each other, and a final grand prix with all nine cars on the track. For both of these events the entrance to the short cut was entirely blocked off.\r\n\r\n##**Approach**\r\n\r\nOur plan going into week four was to direct most of our energy towards the time trials and grand prix race. We already had functional code for the first technical challenge, and, as previously mentioned, it was not necessary to spend time on the second technical challenge as code from the grand prix could be reused for it. The grand prix was what we had to do the most work on and where we dedicated most of our time.  We planned to have two separate programs. One for the time trials and one for the heat races and grand prix.\r\n\r\nFor the time trials we planned to implement a wall following node. This made sense because the goal was to go as fast as possible. Since there were no obstacles, it would be fastest to hug the inside wall to minimize the distance the car needed to travel. The shortcut was the on the left side of the track, so on the shortcut run the car could just hug the left wall for the entire duration of the lap. However, when the blob above the shortcut was red, the car would need to go down the right fork. To implement this, we planned to only look for red blobs larger than a certain size. If one was detected, the car would switch to following the right wall for 10 seconds and then switch back to the left.\r\n\r\nFor the grand prix we wanted to use a 'greatest open space' algorithm. This method takes a laser scan and compares each of the distances to a threshold value (e.g. two meters). If the point is farther than that distance away from the car, it is marked as free space. The algorithm then looks for the greatest group of free points clumped together and steers the car in the direction of it. The greatest open space algorithm was preferable to both wall following and potential fields. This is because in the grand prix the robots had to deal with multiple moving obstacles. The wall following node only scans one side of the track and would be completely oblivious to potential collisions on its other side, and a potential field algorithm would drive the car backward any time it got close to another robot, making it virtually impossible to pass. With greatest free space the car would be aware of its entire surroundings and would not back up when it encountered another car.\r\n\r\n##**Process**\r\n\r\n###**Race Preparation**\r\n\r\nOur carefully planned approach to each component of the final challenge was not strictly followed, which definitely held us back throughout the week. When something was not working, we frantically jumped to a different plan, and if that did not work out either we would switch back. We should have put all of our effort into one program instead of wasting time on several programs that we did not even end up using.\r\n\r\nIn addition to poor decision making, we had several problems with our car's hardware. The ZED camera malfunctioned on multiple occasions and cost us a lot of time. In addition, every time the car ran any form of vision code, the programs would run extremely slowly and sometimes shut down. We had to connect a computer to it with an ethernet cord whenever we needed to test the car, which caused difficulty as someone had to hold a computer and follow very close to the car.\r\n\r\nEventually because of all the issues surrounding blob detection and the time restraints we were under, our team reevaluated and made a new, much simpler plan. We decided to scratch all code concerning blob detection and use the same code for the time trials and the grand prix. Instead of 'greatest open space,' a potential field algorithm would be implemented. Even though this was not our first choice, we had a very good potential field algorithm from the previous week that just needed to be slightly adjusted for the grand prix challenge.\r\n\r\nThe major adjustment we made to the potential field algorithm was add an artificial charge pushing it slightly to the right. This made sure that the car would turn right every time it got to the  fork in the racetrack. In addition we vamped up the propelling charge from three to five and the Alaser from .005 to .01. These were the only changes we made to the algorithm.\r\n\r\n###**Race Day**\r\n\r\nDuring the first technical challenge, another vehicle collided with the side of our car (car 7) and our servo (the steering mechanism on the car) overheated. The combination of these events led to the servo dying; car 7 could no longer turn its wheels. So, an hour before the beginning of the time trials, we had to switch cars and use the instructor's. Because our algorithm had been calibrated for our original car, we were given permission to test on the race track and make necessary adjustments to our code.\r\n\r\n##**Results**\r\n\r\n### **Technical Challenge 1: Exploring Space**\r\n\r\nThe exploring challenge was exactly the same as week three's challenge; the cars just had to drive around an enclosure autonomously, detect blobs, and avoid obstacles. Our car was successful in this challenge. Later several cars were placed in the pen together and our car still managed to avoid all collisions (although other cars did collide with ours).\r\n\r\nFigure 1: Robot Exploring Space and Avoiding Obstacles\r\n\r\n### **Technical Challenge 2: Making the Correct Turn**\r\n\r\nThe second technical challenge was a slightly simplified version of week two's final challenge. The cars had to drive up to a fork in the racetrack, detect a blob, and decide which direction to turn based on its color.\r\n\r\nBecause of the ZED camera malfunctions and how slow all image-related programs were running on our car, my team did not manage to perfect the code for the second technical challenge and were unable to participate. We did get very close to a functional algorithm, and had we been able to test and debug for a longer period of time, our robot most likely would have been able to complete the challenge.\r\n\r\n### **Time Trials**\r\n\r\nThe time trials were the first challenge we had to complete with the new car. During the time trials, cars had three attempts to go around the race track as quickly as possible. Cars were first ranked by number of successful runs, then by time.\r\n\r\n![racetrack](https://cloud.githubusercontent.com/assets/18174572/17789577/d61cab9a-6560-11e6-86b3-74ce04737ddb.png)\r\n\r\nFigure 2: Map of Grand Prix Racetrack. Cars Moved counterclockwise around the course.\r\n\r\nBecause we opted to ignore the shortcut and go the long way for each run, all of our times were within three tenths of a second.\r\n\r\nTable 1: Time Trial Results\r\n\r\n|Lap 1|Lap 2|Lap 3|Best Time|\r\n|-----|-----|-----|---------|\r\n|35.61 |35.63 |35.33 |**35.33** |\r\n\r\nOur fastest lap took the car 35.33 seconds, which earned us third place overall. The time trial results were used to seed cars into rows for the grand prix; first, second, and third place would be in the first row, fourth fifth and sixth in the second, and seventh, eighth, and ninth in the third.\r\n\r\n### **Heat Race and Grand Prix**\r\n\r\nAfter the time trials, groups of three cars raced together to vie for the most advantageous spot in the row. First place in the heat would be on the left, second in the center, and third on the right. Our car was in the first heat with cars 34 and 63 and won second place.\r\n\r\nIn the final grand prix, all nine cars raced the course at the same time. Our car had a rough start and was passed by several cars behind it, but managed to make up a lot of ground during the lap and finished fourth overall, narrowly beating out car 4.\r\n\r\nOverall, we could have been more successful had we stuck to our original plan, but our car performed well and we were proud of the results we achieved.\r\n\r\n***********************\r\n\r\n#**Technical Conclusions**\r\n\r\n##**Results Summary**\r\n\r\nDuring week one my team completed both goals; we became comfortable with ROS, Linux, and Python and we wrote a functional wall following node. Our fastest time achieved during the final challenge was 8.645 seconds and we finished in sixth place.\r\n\r\nThe second week was the least successful, not just for me and my team but for most teams in the program. Only two teams had successful runs in the final challenge. However all of the students definitely completed the first goal of gaining experience in image processing and blob detection.\r\n\r\nI consider week three to be the week that my team was the most successful. Not only did we work cohesively and quickly, we finished the final challenge with 58 points and earned second place.\r\n\r\nAlthough there were some bumps and we were unsuccessful in implementing vision code in either the second technical challenge or the time trials, for the most part week four was a triumph. We placed third in the time trials with a time of 35:33, second in the heat race, and fourth in the grand prix.\r\n\r\n##**Findings**\r\n\r\nOf course, this program taught me a lot about ROS, Python, Linux, and many common algorithms used in programming autonomous vehicles.\r\n\r\nAfter participating in this program, I also have a much better understanding and appreciation of the complexity entailed in programming autonomous vehicles. Even the grossly simplified version of the problem companies like Google are trying to solve that students were faced with at Beaver Works Summer Institute was a formidable challenge.\r\n\r\n##**In the Future**\r\n\r\nI came away from the program with many lessons that will continue to be relevant for the rest of my academic career and beyond. The first of these lessons was the importance of planning. When a course of action is defined before anything is actually started, the work process goes much more smoothly. The plan may change as new information reveals itself, but having the steps to the goal roughly decided ahead of time cuts down the amount of wasted time and helps with organization. I also learned ways to make debugging much easier. If one adds print statements after conditionals and at the start of functions, locating the source of an error becomes simple. Finally, I learned to reach out for help when I needed. Sometimes another pair of eyes is all that is necessary to turn a broken program into a fully functional program.\r\n\r\n*******************************************\r\n\r\n#**Reflections**\r\n\r\n##**Personal**\r\n\r\nThe Beaver Works Summer Institute proved to be an invaluable experience for me. I learned more than I could have ever imagined about communication, about the future, and about myself.\r\n\r\nBefore this program, I neither enjoyed group work nor appreciated its value. For the most part I viewed team projects as a nuisance and did not respect the input of fellow group members. The communication lectures completely reversed my attitude towards working with others. Because of Beaver Works Summer Institute I am now a better listener and I am more aware of other's feelings and the fact that there could be something motivating those feelings that I do not know about.\r\n\r\nI also have a much better sense of what I want to do in the future because of this program. I loved every second of learning about and programming the robots and now feel confident that I want to go into computer science and engineering. My only reservation about these majors in the past was my passion for biology, but the guest seminars showed me that STEM degrees are highly versatile and research often overlaps into multiple fields. For example, Sangbae Kim, one of the speakers, works in MIT's biomimetics lab, where they design robots inspired by biological principles. This program was helpful in deciding what I want to do in college and as a career.\r\n\r\nBWSI was both an empowering and humbling experience. Heading into the program I was nervous that I would be way behind everyone else as I did not get to complete as much of the pre-program material as I would have liked. However, when I arrived I realized that every single student had different strengths and weaknesses, and it was okay that I was not that comfortable with ROS because I knew Python really well. The program made me a lot more confident in my own abilities, but at the same time it humbled me. I realized that there is always someone I can learn from.\r\n\r\n##**Thoughts on Program**\r\n\r\nI have very little to say in terms of how this program could be improved. The one thing that I think the program directors and instructors should consider is keeping the teams the same throughout all four weeks. One thing that Ms. Connor stressed during the communications lectures was the importance of building trust within a team. She also explained that trust takes time to build. When the teams are randomized each week, people have no opportunity to build the strong trust that is necessary for a group to be successful. If the teams were kept the same throughout the month they would be able to work together more successfully.\r\n\r\n******************************\r\n\r\n#**Sources**\r\n\r\n###**Cited**\r\n\r\n(1) Guldner, Owen. (2016) Introduction to the RACECAR Platform [Powerpoint slides]. Retrieved from https://drive.google.com/file/d/0B6jv7Ea8ZHnNZmZTbUdLWktyLW8/view\r\n\r\n(2) \"Jetson TX1 Embedded Systems Module from NVIDIA Jetson.\" _Nvidia._ N.p., n.d. Web. 15 Aug. 2016. <http://www.nvidia.com/object/jetson-tx1-module.html>.\r\n\r\n(3) PID Compensation Animated. Digital image. Commons.wikimedia.org. N.p., 28 May 2015. Web. 13 Aug. 2016. <https://commons.wikimedia.org/wiki/File:PID_Compensation_Animated.gif>.\r\n\r\n(4) Thomas, Dirk. \"Introduction.\" _ROS.org_. 22 May 2014. Web. 12 August 2016. <http://wiki.ros.org/ROS/Introduction>.\r\n\r\n(5) Boulet, Michael T.. (2016) Introduction to ROS [Powerpoint slides]. Retrieved from https://piazza-resources.s3.amazonaws.com/ikimc42bcsv68r/iqjhdwd8mg0c3/B__Introduction_to_ROS_Lecture.pdf?AWSAccessKeyId=AKIAIEDNRLJ4AZKBW6HA&Expires=1471210259&Signature=bMe%2FqyItVgZueiPzbiPhWCLqLYQ%3D\r\n\r\n(6) Edelberg, Kyle. \"Control Systems.\" Beaver Works Summer Institute. The Daniel Guggenheim Aeronautical Laboratory, Boston. 13 July 2016. Lecture.\r\n\r\n(7) \"OpenCV.\" OpenCV. N.p., 19 May 2016. Web. 15 Aug. 2016. <http://opencv.org/>.\r\n\r\n(8) Detry, Renaud. (2016) Image Processing [Powerpoint slides]. Retrieved from https://piazza-resources.s3.amazonaws.com/ikimc42bcsv68r/iqv1h3yfxfl3jp/11imageprocessingcv.pdf?AWSAccessKeyId=AKIAIEDNRLJ4AZKBW6HA&Expires=1471369483&Signature=o4VdJxFZKf5dYsmyXqz2MLrHVAE%3D\r\n\r\n(9) \"How Do We See Color?\" LiveScience. TechMedia Network, n.d. Web. 19 Aug. 2016. <http://www.livescience.com/32559-why-do-we-see-in-color.html>.\r\n\r\n(10) \"HSL and HSV.\" Wikipedia. Wikimedia Foundation, n.d. Web. 19 Aug. 2016. <https://en.wikipedia.org/wiki/HSL_and_HSV>.\r\n\r\n(11) \"Colored Squares.\"\" Digital image. refreshwichitafalls.com. Web. 15 Aug. 2016. <http://refreshwichitafalls.com/images/challenge-foursquare.png>\r\n\r\n(12) \"Colored Shapes.\" Digital image. clear-mind-meditation-techniques.com. Web. 15 Aug. 2016.\r\n<http://www.clear-mind-meditation-techniques.com/image-files/simple-colored-shapes.jpg>\r\n\r\n(13) \"Documentation.\" ROS.org. N.p., 11 Jan. 2016. Web. 13 Aug. 2016. <http%3A%2F%2Fwiki.ros.org%2F>.\r\n\r\n(14) Image retrieved from Friday Lab Challenge sheet: https://docs.google.com/document/d/1tuRuW7xBLRTJqUfpCnNMH-ktWLft_vGdvnr7RT1cgk8/edit\r\n\r\n(15) Karaman, Sertac. (2016) Basics of Autonomy: Localization and Mapping [Powerpoint slides]. Retrieved from https://www.dropbox.com/sh/trzu47z3f27ha27/AAACnQvdlMnihpcdFI_8Zdcna/Lecture%206.pdf?dl=0\r\n\r\n(16) Karaman, Sertac. (2016) Basics of Autonomy: Planning and Control [Powerpoint slides]. Retrieved from  https://www.dropbox.com/sh/trzu47z3f27ha27/AACxX5rGpFfqSECANnGJ6Gqka/Lecture%205.pdf?dl=0\r\n\r\n###**Referenced**\r\n\r\nEdelberg, Kyle. \"Control Systems: Application.\" Beaver Works Summer Institute. The Daniel Guggenheim Aeronautical Laboratory, Boston. 14 July 2016. Lecture.\r\n\r\n",
  "note": "Don't delete this file! It's used internally to help with page regeneration."
}